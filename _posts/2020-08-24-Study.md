---
layout: post
title:  "Loss, Optimizer"
date:   2020-08-24T14:25:52-05:00
author: Sunwoo Lee
categories: Study
cover:  "/assets/activation.png"
---





Activation Function을 통해서 Non_linear하게 만들어준 Output을 실제 값과 어떻게 해야되는지 설명 해보려고 한다. 먼저 **Loss Function**은 학습을 시키기 위해서는 기본적으로 Train data(학습을 시켜야 하는)와 Label data(정답 data)가 있어야 하고 Train data를 학습 시켜서 나오는 예측값과 Label data의 실제값의 차이를 수치화 해주는 함수이다.

예측값과 실제값의 차이 즉, 오차가 클수록 Loss Function의 값이 크고, 오차가 작아질수록 값이 작아진다. 그래서 Loss Function의 값을 최소화하는 Weight와 Bias를 찾아가는게 이상적인 학습이라고 할 수 있다. 





#### -평균제곱오차(MSE)

 ![KakaoTalk_20201207_053915951](https://user-images.githubusercontent.com/47741696/101291791-b4fcc100-384e-11eb-86ed-4a0e62b34b7e.jpg)

**Mean Squared Error**는 가장 기본적인 Loss Function으로 yk는 예측값 tk는 실제값이라고 생각하면 된다. 위의 있는 식 그대로 예측값과 실제값 사이의 평균 제곱 오차를 정의한다. 차가 커질수록 제곱 연산으로 값이 뚜렸해지고 오차가 양수든 음수든 누적 값을 증가시키는 특징을 가진다.







#### -CrossEntropyLoss

![KakaoTalk_20201207_053915827](https://user-images.githubusercontent.com/47741696/101291970-d316f100-384f-11eb-80b9-79de8ba10cd4.jpg)

**CrossEntropyLoss**를 설명하기 전에 **NLLLoss(Negative Log Likelihood loss)**에 대해서 간단히 설명을 하자면 위의 그림처럼 NLLLoss는 정답 class가 2개 이상인 Classification에 유용하다. 각 class의 픽셀값을 Softmax를 해주고 L(y) = -log(y) 해주어서 마지막 layer에 LogSoftmax를 추가 해줘야 되는 특징이 있다.



**CrossEntropyLoss**는 LogSoftmax와 NLLLoss가 합쳐졌다고 볼 수 있다.

 LogSoftmax = log(Softmax)이고 CrossEntropyLoss = NLLLoss(LogSoftmax)라고 보면 좀 이해하기 편할듯하다.  그래서 마지막 layer가 Softmax일 필요가 없다. 





### Optimizer

앞서 Loss Function을 통해서 오차를 찾았다면 오차를 최소화시키는 방향으로 학습을 시켜야되는데 Optimizer를 이용해서 W,b를 최소화시키는 이상적인 학습을 할 수 있다. 

![gradient descent](https://user-images.githubusercontent.com/47741696/101292346-618c7200-3852-11eb-88fe-5d639cdebee2.png)

먼저 여러가지 **Optimizer**를 설명하기 전에 **Gradient Descent(경사 하강법)**에 대해서 간단하게 설명을 하자면 위의 그림의 Gradient를 Loss Function의 기울기라고 하면 기울기를 최소방향 즉, Global cost minimum방향으로 기울기를 이동한다.

![KakaoTalk_20201202_005019571](https://user-images.githubusercontent.com/47741696/101292450-da8bc980-3852-11eb-85a0-a626f543f095.jpg)

위의 식처럼 최소화 방향으로 이동하면서 변하는 기울기(L(Wt))를 r(learning rate)를 곱한값과Wt(Weight)를 이용해서 Wt+1로 Weight Update를 하면서 Optimizer를 한다.



#### BGD(Batch Gradient Descent)

Gradient Descent중에서 BGD는 전체 데이터셋을 한번 다 보고 전체 Loss Function을 구해서 Weight Update를 하고 Update된 Weight로 전체 데이터 셋의 Loss를 구하고 Weight Update를 반복한다.

=>전체 데이터셋을 한번 다 보고 Loss Function을 구하기 때문에 메모리도 많이 쓰고 오래걸린다.



#### SGD(Stochastic Gradient Descent)

BGD가 전체 데이터셋을 다보느라 메모리도 많이 쓰고 오래걸렸기 때문에 데이터 1개를 보고 그거에 대한 Weight Update를 한다. N번의 Update가 끝나면 데이터셋을 Shffle하는 특징을 가지고 있다.



#### Mini-Batch GD

SGD가 데이터를 1개씩 보고 Update를 하지만 이마저도 오래걸리고 너무 작은 데이터를 계속 Update를 하면 성능이 좋지 않아서 데이터를 몇개씩 Mini-Batch를 하여 하나의 Mini-Batch를 보고 평균 Loss Function과 Weight Updatge를 한다. 이 경우에는 한 Epoch마다 Shuffle을 하며 SGD라고 똑같이 불리고 있다.



#### ※  Gradient Descent의 문제점

![KakaoTalk_20201202_005019997](https://user-images.githubusercontent.com/47741696/101292885-8b936380-3855-11eb-9ab3-2a33b4a55fe9.jpg)

실제에서 만나는 문제들은 Loss Fuction이 2차 곡선처럼 깔끔하지 않다. 위의 그림 처럼 기울기가 0이 되는 Minimum이 2개 이상인 경우가 많다. 이런 경우에는 Gradient descent만 사용하면 Local Minimum을 최저점으로 인지하는 경우가 생길 수 있다. 그래서 실제로 만나는 문제들은 Local Minimum이 아닌 Global Minimum을 찾아야 한다.





#### 방향성(Momentum)

#### ![KakaoTalk_20201202_005019669](https://user-images.githubusercontent.com/47741696/101292995-4459a280-3856-11eb-8292-20278b6437f3.jpg)

Global Minimum을 찾기 위해서 Vanilla SGD(일반적인 SGD)는 많이 해매면서 가는 경향을 보이기 때문에 Weight Update를 할 때 이전 방향(Momentum)들을 반영하면 더욱 빠르게 Global Minimum에 도달을 한다. 

![KakaoTalk_20201202_005019471](https://user-images.githubusercontent.com/47741696/101293086-cfd33380-3856-11eb-886f-1d5fed711ad3.jpg)

이렇게 기존의 Weight Update를 하는 식에서 L(Wt)대신에 L(Wt)에 P(얼마나)Vt(움직인 방향)을 더한 Vt+1을 적용시킨다. 



#### Learning rate

![KakaoTalk_20201202_005019382](https://user-images.githubusercontent.com/47741696/101294058-4fadcd80-3858-11eb-9570-f0aa5e92986f.jpg)

**Adagrad(Adaptive Gradient)**는 많이 갔던 방향에 대해 그 영향력을 줄인다고 생각하면 된다. 위의 식을 보면 현재의 위치에서 기울기를 구하고 이를 제곱한 것을 누적하여 Learning rate(r)을 누적의 제곱근으로 나눈다.  Learning rate를 나눠주면 Leaning rate의 값이 유동적으로 변하기 때문에 누적치가 가파르던(많이 변화하지 않은) 방향에서는 값이 클 것이고 완만했던(많이 변화했던) 방향에서는 값이 작을 것이다. 

Adagrad는 학습을 계속 진행하면 Step size(Leaning rate 값)이 너무 줄어드는 문제가 있다. G에 계속 제곱한 값을 넣어주기 때문에 G의 값들은 계속증가하기 때문에 나중에는 Step size가 너무 작아져 거의 움직이지 않는 경우가 나온다.

![KakaoTalk_20201202_005019292](https://user-images.githubusercontent.com/47741696/101294190-1d50a000-3859-11eb-9dfb-39b025940310.jpg)

위의 식은 Adagrad의 단점을 해결하기 위해서 나온 RMSProp라는 방법으로 Adagrad의 식에서 Gt부분을 합이 아니라 지수평균으로 대체한 방법이다. 이렇게 decay rate를 두어서 기존에는 그대로 누적되는 것들을 조금씩 decay하게 만들 경우 Gt가 무한정으로 커지지는 않으면서 최근 변화량의 변수간 상대적인 크기 차이는 유지를 할 수 있다.



#### Adam(Adaptive Moment Estimation)

![KakaoTalk_20201202_005019104](https://user-images.githubusercontent.com/47741696/101294391-5b9a8f00-385a-11eb-9ab0-fef179f06da8.jpg)

Adam은 현재 가장 많이 쓰는 Optimizer로 나도 가장 많이 쓰고 있다. 앞서 설명했던 RMSProp과 Momentum방식을 합친 거처럼 보이는 이 알고리즘은 Momentum 방식과 유사하게 지금까지 계산한 기울기의 지수평균을 저장하여 RMSProp처럼 기울기의 제곱값의 지수평균을 저장한다. 다만 Mt와 Vt가 처음에 0에 가깝게 Bias 되어있을 것이라 판단하여 Unbiased 하게 만들어주는 작업을 거친다.



`※지금까지 설명한 Optimizer들을 실습을 통해서 비교할 예정이다.`

