---
layout: post
title:  "Activation Function 알아보기"
date:   2020-08-18T14:25:52-05:00
author: Sunwoo Lee
categories: Study
cover : "/assets/activation.png"
---



**Activation Function**에 대해서 쉽게 설명부터 하자면 Non_linear가 아닌 선형으로 Weight를 Udate를 해도 치환을 하면 Wx+B라는 동일한 선형 결과를 가져오기 때문에 CNN같이 layer가 깊어지면 선형 Output은 의미가 없어진다. 그래서 **Activation Function**은 Layer에서 나오는 Output을 복잡하게 즉, Non_linear 하게 만들어 준다.  

**`-Sigmoid Function`**

![KakaoTalk_20201202_005018879](https://user-images.githubusercontent.com/47741696/100777017-978fb780-3448-11eb-8901-a48ccb5dbec5.jpg)

Activation Function을 통한 Non_linear가 왜 필요한지 앞서 간단하게 소개를 하였으니 이제는 몇 가지 Activation Function에 대해서 설명을 하려고 한다. 먼저 Sigmoid 함수는 입력을 (0,1) 사이의 값으로 Normailze하는 특징을 가지고 있다. 

위의 그림을 보면 -6, 6 구간부터 접선의 기울기가 0이되는 구간이 시작되는데 Back propagation을 통해서 편미분을 할때 dL/dw가 0이 되면서  weight의 update가 없어져서 saturation현상이 발생한다.  

![vanishing](https://user-images.githubusercontent.com/47741696/100777614-65328a00-3449-11eb-9cb3-72abf2abf687.jpg)

이런 saturation되는 현상을 위의 그림처럼 Vanishing gradient 라고 한다. 그리고 결과값이 0~1사이로만 나와서 모두 같은 부호를 가지기 때문에 zigzag현상이 발생하여 Sigmoid 함수는 현재 많이 사용하지는 않는다.



**`-tanh Fuction`**

![KakaoTalk_20201202_005018788](https://user-images.githubusercontent.com/47741696/100778052-e2f69580-3449-11eb-9732-60e6644fb2b7.jpg)

Sigmoid 함수의 zigzag현상을 해결하기 위해서 결과값이 -1, 1 사이로 나오게 하는 tanh 함수이다 . 결과값이 -1, 1 이라서 zigzag 현상이 줄어들었지만 Vanishing gradient 문제는 계속되어서 많이 사용하지 않는다.



**`-ReLU Function`**

![KakaoTalk_20201202_005018420](https://user-images.githubusercontent.com/47741696/100778681-a7a89680-344a-11eb-9967-df047a3258b0.jpg)

앞서 나온 두가지 함수의 고질적인 문제인 Vanishing gradient를 해결해서 현재 가장 많이 쓰이고 있는 ReLU 함수이다.  그림처럼 음수는 0이고 양수는 x의 값을 가진다. 앞에서 설명한 두개의 함수는 양쪽에서 saturation이 생겼지만 ReLU는 음수에서만 saturation이 안생기고 exp연산이 없기 때문에 매우 빠른 수렴속도를 보인다는 장점을 가지고 있다. 



**`-PReLU, Leaky ReLu`** 



![2](https://user-images.githubusercontent.com/47741696/100779626-0589ae00-344c-11eb-81b5-4cd05512a3df.png)

ReLU는 음수값이 한번 들어오면 Back propagation이 0을 가지기 때문에 노드가 통째로 죽어버리는 Dying ReLU라는 문제점을 가지고 있다. 그래서 위 그림의 PRelu, Leaky ReLU처럼 음수값에 알파값을 주어서 Dying ReLU를 해결하려는 노력을 하는 함수들도 많이 나왔다.

※ 실제 학습을 시키면서 ReLU 대신에 PReLU나 Leaky ReLU를 사용했을 때 눈에 보일 정도의 성능은 보지 못하였고 오히려 ReLU가 더 좋은 성능을 가져오는 경우도 많았다. 관련 논문을 읽어보니 일반 ReLU가 regularzation 경향을 증가시켜 Drop-out과 비슷한 효과를 내는 경우도 있다고 한다.  관련 내용은 fine-tuning 글에서 다시 한번 자세히 설명을 하려고 한다.