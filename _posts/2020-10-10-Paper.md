---
layout: post
title:  "ICNet(ICCV 2018 Paper 논문 리뷰 및 Pytorch 코드 분석)"
date:   2020-10-10T14:25:52-05:00
author: Sunwoo Lee
categories: Paper
---

### ICNet: Real-time Semantic Segmentation on High Resolution Images



#### Introduction

**ICNet : image cascade network** 으로 저번에 포스팅한 PSPNet을 언급하면서 자율주행이나 로봇같은 task에 활용하기 위해서는 real-time이 중요하다라는 방향으로 연구를 하여 accuracy를 최대한 유지하면서 inference 속도를 증가시키는 방법에 대해서 제안한다. 

<br/>

![ICNet0](https://user-images.githubusercontent.com/47741696/102329196-72b04e00-3fcb-11eb-91d3-a47f7069ceb3.PNG)

오른쪽 상단에 보면 수식으로 무엇이 속도에 영향을 주는지에 대해서 표현을 해놨다. Output map인 U를 보면 **커널과 input image가 연산량에 많은 영향을 미치는것을 확인**할 수 있다. 그래서 본 논문에서는 커널과 input size에 대해서 줄이려는 시도를 했지만 커널은 3x3으로 유지를 해야되기 때문에 input image의 크기를 줄여 많은 연산량을 줄였고 이를 통해서 accuracy를 최대한 유지하면서 연산량을 줄였다라는 제안을 한다. 

<br/>

![ICNet1](https://user-images.githubusercontent.com/47741696/102329895-41844d80-3fcc-11eb-9e63-94607966faf2.PNG)

Input image를 줄이면 줄일수록 detail이 사라져 accuracy가 많이 떨어지는게 정상인데 어떤방법으로 accuracy를 유지했는지 확인해보기 위해서는 위에 Architecture를 참고하면 된다. 

그림을 보면 원본, 원본의 1/2, 원본의 1/4 이렇게 3개의 input image가 있고 각자 Convolution을 진행하여 합쳐서 하나의 ouput으로 내보내는 것을 확인할 수 있다. 이렇게 하면 **가장 작은 input image에서는 size가 가장 작기 때문에 detail한 부분을 얻을 수 없지만 semantic parts를 얻을 수 있고 원본 이미지에서는 detail을 얻어서 recover**하는 것을 확인할 수 있다. 

<br/>

- #### Cascade Feature Fusion

  ![ICNet5](https://user-images.githubusercontent.com/47741696/102331426-303c4080-3fce-11eb-8156-d0595efd0ff9.PNG)

위에 그림을 보면 CFF를 확인할 수 있는데 쉽게 말하면 3개로 나눈 Input image들을 Convolution 이후에 cascade feature들을 합치기 위해서 하는 것이다. F1이 가장 작은 feature map이고 F2가 중간 feautre map이다. feature map의 크기를 맞추기 위해서 F1을 upsampling을 하고 이후에 dilated convolution을 한다. F2는 channel을 맞추기 위해서 1x1 convolution을 진행하여 각각 normalization 이후에 sum을 하고 ReLU를 통해서 F2 feature map을 얻게 된다. 

<br/>

```
class CascadeFeatureFusion(nn.Module):
    """CFF Unit"""

    def __init__(self, low_channels, high_channels, out_channels, nclass, norm_layer=nn.BatchNorm2d, **kwargs):
        super(CascadeFeatureFusion, self).__init__()
        self.conv_low = nn.Sequential(
            nn.Conv2d(low_channels, out_channels, 3, padding=2, dilation=2, bias=False),
            norm_layer(out_channels)
        )
        self.conv_high = nn.Sequential(
            nn.Conv2d(high_channels, out_channels, 1, bias=False),
            norm_layer(out_channels)
        )
        self.conv_low_cls = nn.Conv2d(out_channels, nclass, 1, bias=False)

    def forward(self, x_low, x_high):
        x_low = F.interpolate(x_low, size=x_high.size()[2:], mode='bilinear', align_corners=True)
        x_low = self.conv_low(x_low)
        x_high = self.conv_high(x_high)
        x = x_low + x_high
        x = F.relu(x, inplace=True)
        x_low_cls = self.conv_low_cls(x_low)

        return x, x_low_cls
```

forward 부분을 살펴보면 x_low(F1)을 interpolate로 2배로 upsampling을 한 이후에 x_high(F2)와 합쳐서 최종 output을 내보내는 것을 확인할 수 있다.

<br/>

- #### Cascade Label Guidance

Input image가 나눠지기 때문에 learning procedure이 약해져서 Loss를 마지막에만 계산하지 않고 중간중간에 Loss를 구한다음에 각 Loss에 weight를 곱해서 어느 Loss에 더 가중을 둬야하는지 결정한다.

```
class ICNetLoss(nn.CrossEntropyLoss):
    """Cross Entropy Loss for ICNet"""
    
    def __init__(self, aux_weight=0.4, ignore_index=-1):
        super(ICNetLoss, self).__init__(ignore_index=ignore_index)
        self.aux_weight = aux_weight

    def forward(self, *inputs):
        preds, target = tuple(inputs)
        inputs = tuple(list(preds) + [target])

        pred, pred_sub4, pred_sub8, pred_sub16, target = tuple(inputs)
        # [batch, H, W] -> [batch, 1, H, W]
        target = target.unsqueeze(1).float()
        target_sub4 = F.interpolate(target, pred_sub4.size()[2:], mode='bilinear', align_corners=True).squeeze(1).long()
        target_sub8 = F.interpolate(target, pred_sub8.size()[2:], mode='bilinear', align_corners=True).squeeze(1).long()
        target_sub16 = F.interpolate(target, pred_sub16.size()[2:], mode='bilinear', align_corners=True).squeeze(
            1).long()
        loss1 = super(ICNetLoss, self).forward(pred_sub4, target_sub4)
        loss2 = super(ICNetLoss, self).forward(pred_sub8, target_sub8)
        loss3 = super(ICNetLoss, self).forward(pred_sub16, target_sub16)
        #return dict(loss=loss1 + loss2 * self.aux_weight + loss3 * self.aux_weight)
        return loss1 + loss2 * self.aux_weight + loss3 * self.aux_weight
```

처음에는 이해가 어렵기 때문에 코드로 설명을 하자면 위에 처럼 loss1, loss2, loss3을 각각 3개를 따로 구해서 최종 loss는  loss1 + loss2 * self.aux_weight + loss3 * self.aux_weight로 가중치를 합해서 내보낸다. 이 과정에서 **상대적으로 큰 aux_weigth가 곱해진 loss는 전체 loss를 줄이기 위해서 다른 loss보다 더 많이 줄여지게 된다**. 어디 가중치를 더 주냐에 따라서 detail한 부분과 전체적인 부분의 성능을 더 높일 수 있다. 

![ICNet3](https://user-images.githubusercontent.com/47741696/102345580-f7599700-3fe0-11eb-8163-6add04497a32.PNG)

ICNet은 30 Frame을 유지하면서 70이라는 높은 mIoU 결과를 낼 수 있었다. ICNet의 실제 사용이 궁금하다면 <>에 ICNet, PSPNet, ESPNet의 결과를 비교했으니 확인해보면 된다.