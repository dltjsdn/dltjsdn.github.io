---
layout: post
title:  "Activation Function 알아보기"
date:   2020-08-18T14:25:52-05:00
author: Sunwoo Lee
categories: Study
cover:  "/assets/activation.png"
---



**Activation Function**에 대해서 쉽게 설명부터 하자면 Non_linear가 아닌 선형으로 Weight를 Udate를 해도 치환을 하면 Wx+B라는 동일한 선형 결과를 가져오기 때문에 CNN같이 layer가 깊어지면 선형 Output은 의미가 없어진다. 그래서 **Activation Function**은 Layer에서 나오는 Output을 복잡하게 즉, Non_linear 하게 만들어 준다.  





#### **`-Sigmoid Function`**

![KakaoTalk_20201202_005018879](https://user-images.githubusercontent.com/47741696/100777017-978fb780-3448-11eb-8901-a48ccb5dbec5.jpg)

Activation Function을 통한 Non_linear가 왜 필요한지 앞서 간단하게 소개를 하였으니 이제는 몇 가지 Activation Function에 대해서 설명을 하려고 한다. 먼저 Sigmoid 함수는 입력을 (0,1) 사이의 값으로 Normailze하는 특징을 가지고 있다. 

위의 그림을 보면 -6, 6 구간부터 접선의 기울기가 0이되는 구간이 시작되는데 Back propagation을 통해서 편미분을 할때 dL/dw가 0이 되면서  weight의 update가 없어져서 saturation현상이 발생한다.  

![vanishing](https://user-images.githubusercontent.com/47741696/100777614-65328a00-3449-11eb-9cb3-72abf2abf687.jpg)

이런 saturation되는 현상을 위의 그림처럼 Vanishing gradient 라고 한다. 그리고 결과값이 0~1사이로만 나와서 모두 같은 부호를 가지기 때문에 zigzag현상이 발생하여 Sigmoid 함수는 현재 많이 사용하지는 않는다.







#### **`-tanh Fuction`**

![KakaoTalk_20201202_005018788](https://user-images.githubusercontent.com/47741696/100778052-e2f69580-3449-11eb-9732-60e6644fb2b7.jpg)

Sigmoid 함수의 zigzag현상을 해결하기 위해서 결과값이 -1, 1 사이로 나오게 하는 tanh 함수이다 . 결과값이 -1, 1 이라서 zigzag 현상이 줄어들었지만 Vanishing gradient 문제는 계속되어서 많이 사용하지 않는다.







#### **`-ReLU Function`**

![KakaoTalk_20201202_005018420](https://user-images.githubusercontent.com/47741696/100778681-a7a89680-344a-11eb-9967-df047a3258b0.jpg)

앞서 나온 두가지 함수의 고질적인 문제인 Vanishing gradient를 해결해서 현재 가장 많이 쓰이고 있는 ReLU 함수이다.  그림처럼 음수는 0이고 양수는 x의 값을 가진다. 앞에서 설명한 두개의 함수는 양쪽에서 saturation이 생겼지만 ReLU는 음수에서만 saturation이 안생기고 exp연산이 없기 때문에 매우 빠른 수렴속도를 보인다는 장점을 가지고 있다. 







#### **`-PReLU, Leaky ReLu`** 



![2](https://user-images.githubusercontent.com/47741696/100779626-0589ae00-344c-11eb-81b5-4cd05512a3df.png)

ReLU는 음수값이 한번 들어오면 Back propagation이 0을 가지기 때문에 노드가 통째로 죽어버리는 Dying ReLU라는 문제점을 가지고 있다. 그래서 위 그림의 PRelu, Leaky ReLU처럼 음수값에 알파값을 주어서 Dying ReLU를 해결하려는 노력을 하는 함수들도 많이 나왔다.

**※** 실제 학습을 시키면서 ReLU 대신에 PReLU나 Leaky ReLU를 사용했을 때 눈에 보일 정도의 성능은 보지 못하였고 오히려 ReLU가 더 좋은 성능을 가져오는 경우도 많았다. 관련 논문을 읽어보니 일반 ReLU가 regularization 경향을 증가시켜 Drop-out과 비슷한 효과(**overfitting 방지**)를 내는 경우도 있다고 한다.  관련 내용은 fine-tuning 글에서 다시 한번 자세히 설명을 하려고 한다.







#### **`-Swish, Mish`**

![KakaoTalk_20201202_005018269](https://user-images.githubusercontent.com/47741696/100780808-8ac19280-344d-11eb-97b6-1a4254a81593.jpg)

최근에는 ReLU의 PReLU, Leaky ReLU의 장점을 가지고 있는 Swish나 mish가 좋은 성능을 보인다는 논문을 많이 볼 수 있다. Swish는 그림처럼 음수쪽이 smooth한 형태를 가지고 있기 때문에 기존의 ReLU보다 초기값과 learning rate에 덜 민감하고 하한이 있기 때문에 강한 regularization효과를 가진다. 그래서 매우 깊은 신경망에서는 ReLU보다 높은 정확도를 달성한다고 한다.



![mish](https://user-images.githubusercontent.com/47741696/100780870-a331ad00-344d-11eb-866b-3dbda2ee1913.png)

위의 Mish는 내 연구에서 최근 가장 많이 쓰고 있는 activation Function이다. swish와 마찬가지고 하한이 있기 때문에 강한 regularization 효과를 주는 Bounded Blow 이고 약간의 음수를 허용하기 때문에 그라디언트를 잘 흐르게 한다는 특징이 있다. 

`**※ Mish 관련 연구를 진행하고 있어서 실습 카테고리에서 Mish 적용한 결과를 글로 적으면서 더욱 자세히 설명을 할 예정이다.****`

